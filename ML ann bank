# 1. Import libraries
import pandas as pd                      # dataframes
import numpy as np                       # numeric ops
from sklearn.model_selection import train_test_split  # split data
from sklearn.preprocessing import StandardScaler      # feature scaling
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.neural_network import MLPClassifier      # neural network model
import seaborn as sns                    # plotting heatmap
import matplotlib.pyplot as plt          # plotting

# 2. Load dataset
df = pd.read_csv("a3.csv")               # read CSV into DataFrame
print(df.head())                         # show first 5 rows to inspect

# 3. Separate features and target
# Drop non-feature ID/text columns and the target 'Exited'
X = df.drop(['RowNumber', 'CustomerId', 'Surname', 'Exited'], axis=1)
y = df['Exited']                         # target: 1 => left, 0 => stayed

# 4. Encode categorical variables (one-hot)
X = pd.get_dummies(X, drop_first=True)   # convert 'Geography' and 'Gender' to numeric columns

# 5. Split data into training and testing sets
# 80% train, 20% test, fixed random_state for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 6. Normalize (scale) the data for stable and fast training
scaler = StandardScaler()                 # create scaler instance
X_train = scaler.fit_transform(X_train)   # fit on train and transform train
X_test = scaler.transform(X_test)         # transform test using same scaler

# 7. Build and train the Neural Network model (fix convergence)
model = MLPClassifier(
    hidden_layer_sizes=(64, 32, 16),      # 3 hidden layers: 64 -> 32 -> 16 neurons
    activation='relu',                    # ReLU activation
    solver='adam',                        # Adam optimizer
    alpha=0.001,                          # L2 regularization strength
    learning_rate='adaptive',             # reduce LR if validation score stops improving
    early_stopping=True,                  # stop when no improvement on validation set
    max_iter=500,                         # max epochs (increased to avoid early convergence warning)
    random_state=42
)
===============================================================================
Creates a Multi-Layer Perceptron (MLP) classifier.

Architecture: 3 hidden layers (64, 32, 16 neurons).

Activation: 'relu' ‚Üí fast and efficient for deep networks.

Solver: 'adam' ‚Üí adaptive gradient descent optimizer.

Alpha: L2 regularization term to avoid overfitting.

learning_rate='adaptive'` ‚Üí automatically adjusts learning rate.

early_stopping=True ‚Üí stops when performance stops improving (avoids overtraining).

max_iter=500 ‚Üí allows enough training epochs.

random_state=42 ‚Üí ensures reproducibility.
====================================================================================
model.fit(X_train, y_train)                # train the model on training data

# 8. Make predictions on the test set
y_pred = model.predict(X_test)             # predicted labels for test set

# 9. Evaluate model performance
acc = accuracy_score(y_test, y_pred)       # overall accuracy
cm = confusion_matrix(y_test, y_pred)      # confusion matrix
print("\nModel Accuracy:", round(acc, 3))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 10. Visualize confusion matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()


========================================================================================================


---

### Assignment (short)

Predict whether a bank customer will leave (churn) in the next 6 months using a neural-network classifier. Steps: load data, separate features/target, encode categorical features, split data, scale features, build/train MLP, evaluate with accuracy and confusion matrix.

---

### Output (in short)

* **Model Accuracy:** single number (e.g., 0.86) ‚Äî percentage of correct predictions on test set.
* **Classification report:** precision, recall, F1 for both classes (0 = stayed, 1 = left).
* **Confusion matrix:** counts of true positives, false positives, true negatives, false negatives shown as a heatmap.

Interpretation: higher precision for class 1 means fewer false spam-like predictions; higher recall for class 1 means more churners correctly detected. Use both metrics to judge model usefulness.

---

### Theory (simple)

* **Problem type:** Binary classification (churn vs no-churn).
* **Model:** MLP (Multi-Layer Perceptron) ‚Äî feedforward neural network with hidden layers that learn non-linear feature interactions.
* **Why scale:** NN and gradient-based solvers converge faster and more reliably when features are standardized.
* **Regularization & early stopping:** reduce overfitting and stop training once validation performance stops improving.

---

### Viva Q&A (very short)

Q: What is the target label?
A: `Exited` (1 = left, 0 = stayed).

Q: Why one-hot encode categorical features?
A: To convert text categories to numeric features suitable for the model.

Q: Why use StandardScaler?
A: To standardize features so gradient descent behaves well.

Q: What does `early_stopping=True` do?
A: It stops training automatically when validation score stops improving.

Q: How do you judge model performance?
A: Use accuracy, precision, recall, F1-score, and the confusion matrix.

---

### Time and space complexity (simple)

* **Training time (MLP):** roughly O(epochs √ó n_samples √ó n_features √ó n_neurons). Complexity grows with dataset size, features, layers, and epochs.
* **Prediction time:** O(n_samples √ó n_neurons) per forward pass (fast).
* **Space:** O(n_weights) to store model parameters plus O(n_samples √ó n_features) for data in memory.

---

### Applications (simple)

* Bank customer churn prediction.
* Subscription cancellations.
* Predicting defaults or product returns.
* Any binary decision based on customer features.


üß† What is an Artificial Neural Network (ANN)?
An ANN is a computer model inspired by how the human brain works.
It is made up of neurons (nodes) that are connected by weights.
Each neuron receives inputs, multiplies them by weights, sums them up, and passes the result through an activation function to produce an output.

‚öôÔ∏è Structure of an ANN
Input Layer ‚Äì takes the input features (data values).
Hidden Layer(s) ‚Äì processes data and learns patterns.
Output Layer ‚Äì gives the final prediction or class.
Each layer passes information forward through the network.

üéØ Training the Network
ANN learns using supervised training ‚Äî the correct outputs (labels) are known.
The model makes predictions, compares them with the correct answers, and computes errors.
These errors are sent backward through the network to adjust weights ‚Äî this is called backpropagation.
The process repeats many times until errors are small and the network learns well.

üîÅ Feedforward and Backpropagation
Feedforward: Data flows from input ‚Üí hidden ‚Üí output to make predictions.
Backpropagation: The error flows backward to update weights using the Delta Rule (adjust weights in proportion to error).
This loop continues until the model performs well.

‚öñÔ∏è Designing the Network
The number of layers and neurons depends on the problem‚Äôs complexity.
More complex data ‚Üí more neurons/layers.
Too many neurons ‚Üí model memorizes training data (overfitting).
Use enough training data to help the network generalize, not just memorize.

‚úÖ Advantages
Can learn complex and non-linear relationships.
Works well even with noisy or incomplete data.
Can classify unseen patterns once trained.

üí° In short
An ANN mimics the brain: it takes inputs, learns patterns by adjusting weights through trial and error (training), and produces accurate outputs after learning through feedforward + backpropagation
